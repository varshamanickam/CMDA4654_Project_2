---
title: "Multinomial Regression"
author: "Chris Ryan, Jeb Brown, Varsha Manickam, Meghna Kommoju, Zhennan Shen"
date: "2025-04-22"
fontsize: 9pt
classoption: "aspectratio=169"
output: 
  beamer_presentation: 
    fonttheme: professionalfonts
    highlight: kate
    theme: Rochester
header-includes:
- \definecolor{VTmaroon}{HTML}{861F41}
- \usecolortheme[named=VTmaroon]{structure}
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE,
                      comment = NA,
                      warning = FALSE,
                      message = FALSE)

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  paste0("\n \\", "footnotesize","\n\n", x, "\n\n \\normalsize")
})


```

## Multinomial Regression Introduction

Multinomial Regression (also called Multinomial Logistic Regression or Softmax Regression) is a classification method that is used to predict nominal outcome variables that have more than two outcomes (polytomous) that do not have a rank or order. Examples of this would be:

- Classifying flower species (setosa, virginica, veritosa)
- Classifying student programs (academic, vocation, general)
- Classifying student majors (engineering, business, science)

## Assumptions for Multinomial Regression

-   Independence of observations
-   Categories of the outcome variable must be mutually exclusive and exhaustive
-   No multicollinearity between independent variables
-   Linear relationship between continuous variables and the logit transformation of the outcome variable

## Why This Matters in Multinomial Logistic Regression

- In multinomial regression, we estimate \( K-1 \) sets of coefficients \( \beta_i \) for each non-reference category.
- Each set is estimated using **maximum likelihood**, which assumes stable and independent information from predictors.
- If predictors are highly correlated:
  - The model may assign unstable or misleading weights to them.
  - Standard errors will be large, making **significance tests unreliable**.
  - The optimization (Newton-Raphson updates) may fail to converge or return inflated coefficients.

Checking multicollinearity ensures:
- The **log-odds interpretation** of each \( \beta_{i,j} \) is trustworthy.
- The probabilities returned by the **softmax function** are stable.
- The model doesnâ€™t **overfit noise** from redundant features.

## Multinomial Regression Basics

-   An extension of Logistic Regression
-   Uses separate Logistic Regression models for each pair of response categories
-   Say there are K Response categories. We choose 1 base or "reference" level and then create K-1 different ordinary logistic regression models.
-   Each of these models will model the log-odds that an observation that an observation is in a given category vs the reference category
- The reference category, mathematically, can be chosen to be any of the categories and you will still get the same results. But in practice, we typically choose the most common or basic category as this aids in interpretability.

## Multinomial Regression Interpretation

A Multinomial Regression model with K response categories and P explanatory variables is comprised of K-1 Logistic Regression Models:

$\begin{cases}$
$log(\pi_1/\pi_K) = \beta_{1,0} + \beta_{1,1}*X_1 + ... + \beta_{1,p}*X_p = X^T\beta_1 \\$
$log(\pi_2/\pi_K) = \beta_{2,0} + \beta_{2,1}*X_1 + ... + \beta_{2,p}*X_p = X^T\beta_2 \\$
$... \\$
$log(\pi_{K-1}/\pi_K) = \beta_{K-1,0} + \beta_{K-1,1}*X_1 + ... + \beta_{K-1,p}*X_p = X^T\beta_{K-1}$
$\end{cases}$

- (Where $\pi_i$ is the probability that the observation is in category i and, by convention, category K is our reference category.)

##

As you can see, the odds that a given observation is in category i vs category K is given by $exp(X^T\beta_i)$ and the percentage that a given observation is in category i, $\pi_i$, is $\pi_Kexp(X^T\beta_i)$. 

Since $1 = \sum_{i=1}^{K}\pi_i = \pi_K(1 + \sum_{i=1}^{K-1}exp(X\beta_i))$, $\pi_K = 1/(1 + \sum_{i=1}^{K-1}exp(X^T\beta_i))$. 

Therefore, the probabilities that an observation is in each category are modeled by the following set of equations:
$\begin{cases}$
$\pi_1 = exp(X^T\beta_1)/(1 + \sum_{i=1}^{K-1}exp(X^T\beta_i)) \\$
$... \\$
$\pi_{K-1} = exp(X^T\beta_{K-1})/(1 + \sum_{i=1}^{K-1}exp(X^T\beta_i)) \\$
$\pi_{K} = 1/(1 + \sum_{i=1}^{K-1}exp(X^T\beta_i))$
$\end{cases}$

## Softmax function

The Softmax function is essentially an extension of the logistic function from ordinary logistic regression. The Softmax function takes in a vector of log odds and converts it into a vector of probabilities. Given a vector z of length n, $softmax(z) = (1/\sum_{i=1}^{n}(exp(z_i))) \begin{bmatrix} exp(z_1)\\...\\exp(z_n) \end{bmatrix}$.

Now if we define another parameter vector, $\beta_K$, and set it equal to the zero vector, it is easy to see that $exp(X^T\beta_K) = 1$. We then see that $\pi_K = 1/(1 + \sum_{i=1}^{K-1}exp(X^T\beta_i)) = exp(X^T\beta_K)/(\sum_{i=1}^{K}exp(X^T\beta_i))$ and in general, we can model the probability that the observation is in category i by: $\pi_i = exp(X^T\beta_i)/(\sum_{j=1}^{K}exp(X^T\beta_j))$.

Let's then define $\beta = [\beta_1, \beta_2,...,\beta_K] \in \mathbb{R}^{P{\times}K}$ and $\pi = [\pi_1, ..., \pi_K]^T$. Observe that $\beta^TX = [X^T\beta_1, X^T\beta_2, ..., X^T\beta_K]^T$, meaning, our model can be expressed simply as $\pi = softmax(\beta^TX)$

## Non-Uniqueness of Parameter Estimates

Lets say we update each $\beta_i$ to $\beta_i + \beta^*$ for some constant vector $\beta^*$. Let $pi_i^*$ be the updated probability that an observation is in category i given the updated parameters.

Observe that, $\pi_i^* = exp(X^T(\beta_i + \beta^*))/(\sum_{j=1}^{K}exp(X^T(\beta_j + \beta^*))) = exp(X^T\beta_i)exp(X^T\beta^*)/(\sum_{j=1}^{K}exp(X^T\beta_j)exp(X^T\beta^*)) = exp(X^T\beta_i)/(\sum_{j=1}^{K}exp(X^T\beta_j)) = \pi_i$.

Therefore, as we can see, the probabilities are not uniquely defined by the set of parameters and in fact, for any model if you add the same vector to each of the parameter vectors, you will achieve the same results. This is why we choose a category to be the reference category (setting it's parameter vector to the zero vector) as this restricts our solution space to one solution.

## Interpretation of Parameter Estimates

Let $\beta_{i,j}$ be the parameter corresponding to the coefficient of $X_j$ in the ith logistic regression model.A unit increase in $X_j$ results in a total increase in the log-odds by $\beta_{i,j}$. (If all other explanatory variables are kept constant)

However, similar to Logistic Regression, it is easier to interpret odds than log odds. Therefore, for a unit increase in $X_j$, the odds that a given observation is category i vs our reference category changes by a factor of $exp(\beta_{i,j})$.

## Parameter Estimation

Like ordinary logistic regression, in order to estimate the parameters, $\beta$, we have to calculate the maximum likelihood estimation.

For starters, our likelihood function is given by, $L(\beta|X) = \prod_{i=1}^{n}\prod_{c=1}^{K}P(y_i=c|x_i,\beta)^{1\{y_i=c\}}$. Where $1\{\}$ is the indicator function, which returns 1 if the statement inside the brackets is true and 0 if the statement inside the brackets is false. 

Of course, it is typically easier to differentiate a sum rather than a product so we look at the log-likelihood function instead given by, $\ell(\beta|X) = \sum_{i=1}^n\sum_{c=1}^K1\{y_i=c\}log(P(y_i=c|x_i,\beta) = \sum_{i=1}^n\sum_{c=1}^K1\{y_i=c\}(x_i^T\beta_c - log(\sum_{j=1}^Kexp(x_i^T\beta_j)$.

##

Now, finding the maximum likelihood estimation is equivalent to finding when each $\nabla_{\beta_m}\ell(\beta|X)$ is equal to the zero vector. It can be shown that 
$\nabla_{\beta_m}\ell(\beta|X) = X^T \begin{bmatrix}1\{y_1=m\}-P(y_1=m|x_1,\beta) \\ ... \\ 1\{y_n=m\}-P(y_n=m|x_n,\beta)\end{bmatrix}$

Much like Logistic Regression, the solution to this has no closed form when set equal to 0. Therefore we need to use a generalization of newtons method in order to find the minimum of $\nabla_{\beta_i}\ell(\beta|X)$, using the following update formula: $\beta_m^{(r)} = \beta_m^{(r-1)} - (H(\nabla_{\beta_m}\ell(\beta^{(r-1)}|X))(\nabla_{\beta_m}\ell(\beta^{(r-1)}|X))$

(Where $H(\nabla_{\beta_m}\ell(\beta^{(r-1)}|X)$ is the Hessian of $\nabla_{\beta_m}\ell(\beta^{(r-1)}|X)$. Explicitly, we can deduce that this is defined as $-X^TW_m^{(r-1)}X$, where $W_m$ is an n by n, diagonal matrix with the ith diagonal entry being $P(y_i = m|x_i,\beta^{(r-1)})(1 - P(y_i = m|x_i,\beta^{(r-1)}))$

## Example on a small dataset

Below we will run multinomial regression on the iris dataset to try and predict each observations species. We select the setosa species to be our base level

```{r, message=FALSE, warning=FALSE, echo=TRUE, results='hide'}
library(nnet)

data(iris)
# set base level
iris$Species <- relevel(iris$Species, ref = "setosa")

model <- multinom(Species ~ ., data = iris, blahblahblah=setosa)

```

```{r, echo=TRUE}
model_summary <- summary(model)
model_summary$coefficients
```

Shown above are the coefficients for each predictor variable. Looking at versicolor and Sepal.Length for example, we see that if Sepal Length is increased by one unit, the odds that the flower is a versicolor vs a setosa is decreased by a factor of $exp(-5.458424)=0.004260265$. In other words, the odds will be decreased 99.57397%.

## Why Significance Test in Multinomial Regression?

- Helps determine which predictors are statistically meaningful.
- Prevents overfitting by identifying unhelpful variables.
- **Wald Test**: Used to test individual predictor coefficients.
  - But: Wald tests can be unstable, especially with small samples or multicollinearity.
- **Likelihood Ratio Test (LRT)** is often more reliable.

## Likelihood Ratio Test

- Compares a full model (with all predictors) to a reduced model (with some predictors removed).
- **Null hypothesis**: The reduced model fits the data just as well as the full model.
- **Alternative hypothesis**: The full model fits significantly better than the reduced model.
- A small *p*-value â†’ removed predictors are important.

## LRT Example on `iris` Dataset

```{r, echo=TRUE}
null_model <- multinom(Species ~ 1, data=iris)
lrt_result <- anova(null_model, model, test = "Chisq")
lrt_result
```

- LR stat = 317.684, df = 8, p-value = 0
- p < 0.05 â†’ statistically significant
- There is evidence that the full model is better than the null model

## Main example

```{r include=FALSE}

# Loading necessary libraries and the dataset
library(nnet)
library(caret)
library(readr)
library(dplyr)
 
steel_faults <- read_csv("steel_faults.csv")
 
# We need to combine the various fault columns into one column, and get rid of the separate columns
steel_faults <- steel_faults %>%
 mutate(Fault_Type = case_when(
   Pastry == 1 ~ "Pastry",
   Z_Scratch == 1 ~ "Z_Scratch",
   K_Scatch == 1 ~ "K_Scatch",
   Stains == 1 ~ "Stains",
   Dirtiness == 1 ~ "Dirtiness",
   Bumps == 1 ~ "Bumps",
   Other_Faults == 1 ~ "Other"
 )) %>%
 select(-Pastry, -Z_Scratch, -K_Scatch, -Stains, -Dirtiness, -Bumps, -Other_Faults)
 
 
# We need to convert the targets to factors since we are dealing with categories
steel_faults$Fault_Type <- as.factor(steel_faults$Fault_Type)
 
```
The data we are using for our main example is a dataset that contains various physical and visual measurements of steel plates. These measurements are then used to classify the fault that has occurred in the manufacturing of each plate. 
 
The types of faults that we will be classifying are:
 
- Pastry
- Z Scratch
- K Scratch
- Stains
- Dirtiness
- Bumps
- Other Faults
 
The data was found at https://archive.ics.uci.edu/dataset/198/steel+plates+faults. In our example our goal will to be to build a multinomial regression model that can accurately classify the types of faults that the plates in the dataset have.
 
## Descriptive Statistics
 
Our dataset contains 27 different predictor variables that describe the features of the fault such as size, placement, and luminosity of the faults, and other information such as the type and thickness of the steel that the plate is made of.

Here we can see the distribution of the types of faults in our dataset. 
```{r}
table(steel_faults$Fault_Type)
```
We have a total of 1940 observations of steel plate faults in our dataset.

Multinomial logistic regression is ideal here because:
- The response is categorical with >2 classes (Pastry, Z_Scratch, etc)
- The resulting model can help quality control teams identify the likely cause of fault, enabling targeted improvements in production.

## Building the full model

```{r}
library(MASS)
library(nnet)
```

 
```{r, echo=TRUE}

# Fitting the multinomial model to the data
full_model <- multinom(Fault_Type ~ ., data = steel_faults)

# Storing model summary so we can access various pieces of it 
full_model_summary <- summary(full_model)

# Accessing the AIC of the full model
full_model_summary$AIC
```

## Confusion Matrix and Interpretations
```{r}
# Find predicted values that the model suggests
predicted_full_model <- predict(full_model, newdata = steel_faults)

# Confusion matrix
cm <- confusionMatrix(predicted_full_model, steel_faults$Fault_Type)
cm$overall
cm$table
```


## Confusion Matrix and Interpretations (cont.)
```{r}
cm$byClass
```

## Finding Which Predictors to Use

```{r, echo=TRUE, results='hide'}
# Stepwise variable selection
stepwise_model <- stepAIC(full_model, direction = "both", trace = FALSE)
```

```{r, echo=TRUE}
# See which predictors were selected
stepwise <- summary(stepwise_model)
stepwise$call
```

In the stepwise summary, we are able to see that the model with the lowest AIC is one that contains 20 of our predictor variables. We will now use these predictors to form our refined model.


## Creating Refined Model Using AIC Comparisons
```{r, results='hide', echo=TRUE}
# Fitting the refined multinomial model
refined_model <- multinom(formula = Fault_Type ~ X_Minimum + Y_Minimum + Y_Maximum + 
    Minimum_of_Luminosity + Maximum_of_Luminosity + Length_of_Conveyer + 
    TypeOfSteel_A300 + TypeOfSteel_A400 + Steel_Plate_Thickness + 
    Edges_Index + Empty_Index + Square_Index + Edges_X_Index + 
    Edges_Y_Index + Log_X_Index + Log_Y_Index + Orientation_Index + 
    Luminosity_Index + SigmoidOfAreas + Outside_Global_Index, 
    data = steel_faults)

# Storing model summary so we can access various pieces of it 
refined_model_summary <- summary(refined_model)

# Accessing AIC of the refined model
refined_model_summary$AIC

```

## Updated Confusion Matrix and Interpretations
```{r}
# Find predicted values that the model suggests
predicted_refined_model <- predict(refined_model, newdata = steel_faults)

cm_refined <- confusionMatrix(predicted_refined_model, steel_faults$Fault_Type)
cm_refined$overall
cm_refined$table
```

## Updated Confusion Matrix and Interpretations (cont.)
```{r}
cm_refined$byClass
```

## Final Model Performance: Key Takeaways
- The refined model improved overall classification accuracy from **71.5%** to **73.6%**.
- Kappa increased slightly from **0.62** to **0.66**, indicating better agreement beyond chance.
- **F1 Scores** were highest for:
  - *K_Scratch* (**0.93**): Model is very effective at identifying this fault.
  - *Stains* (**0.91**): Strong performance despite smaller class size.
- **F1 Scores** were lowest for:
  - *Pastry* and *Dirtiness*: These faults may have overlapping features with other categories.
  - The **Other** category continues to show frequent misclassification, probably because it's a catch-all group
  - *Stains* (**0.91**): Strong performance despite smaller class size.

## References
- https://bookdown.org/sarahwerth2024/CategoricalBook/multinomial-logit-regression-r.html
- http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/

- UCLA Institute for Digital Research and Education (IDRE). *Multinomial Logistic Regression | Stata/Logistic Regression Resources*.  
  Retrieved from: [https://stats.oarc.ucla.edu/](https://stats.oarc.ucla.edu/)

- Fox, J., & Weisberg, S. (2019). *An R Companion to Applied Regression* (3rd ed.).  
  Retrieved from: [https://cran.r-project.org/web/packages/car/index.html](https://cran.r-project.org/web/packages/car/index.html)

- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning* (2nd ed.). Springer.  
  Chapter 3 covers multicollinearity and variable selection.