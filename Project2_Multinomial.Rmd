---
title: "Multinomial Regression"
author: "Chris Ryan, Jeb Brown, Varsha Manickam, Meghna Kommoju, Zhennan Shen"
date: "2025-04-22"
fontsize: 9pt
classoption: "aspectratio=169"
output: 
  beamer_presentation: 
    fonttheme: professionalfonts
    highlight: kate
    theme: Rochester
header-includes:
- \definecolor{VTmaroon}{HTML}{861F41}
- \usecolortheme[named=VTmaroon]{structure}
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE,
                      comment = NA,
                      warning = FALSE,
                      message = FALSE)

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  paste0("\n \\", "footnotesize","\n\n", x, "\n\n \\normalsize")
})


```
## Introduction

> "All models are wrong, but some are useful."  
> — George E. P. Box

- In manufacturing, it’s important to find problems early to keep products high quality.
- We are using **Multinomial Logistic Regression** to predict **types of faults** in steel plates using data from the production process.
- The outcome has **more than two categories** with no specific order, which fits this model well.
- Our goal is to build a model that predicts fault types, explains which features matter most, and checks that the model works correctly.

## Multinomial Regression

Multinomial Regression (also called Multinomial Logistic Regression) is a classification method that is used to predict nominal outcome variables that have more than two outcomes (polytomous) that do not have a rank or order.
Examples of this would be:

- Classifying flower species (setosa, virginica, veritosa)
- Classifying student programs (academic, vocation, general)
- Classifying student majors (engineering, business, science)

## Assumptions for Multinomial Regression

-   Independence of observations
-   Categories of the outcome variable must be mutually exclusive and exhaustive
-   No multicollinearity between independent variables
-   Linear relationship between continuous variables and the logit transformation of the outcome variable

## Multinomial Regression Basics

-   An extension of Logistic Regression
-   Uses separate Logistic Regression models for each pair of response categories
-   Say there are K Response categories. We choose 1 base or "reference" level and then create K-1 difference Logistic Regression Models comparing
-   Each of these models will model the log-odds that an observation that an observation is in a given category vs the reference category
- The reference category can be any of the categories mathematically and you will get the same results. But it can strategically be picked for better interpretability.

## Multinomial Regression Interpretation

- A Multinomial Regression model with K response categories and P explanatory variables is a collection of K-1 Logistic Regression Models:
  - $log(\pi_2/\pi_1) = \beta_{2,0} + \beta_{2,1}*X_1 + ... + \beta_{2,p}*X_p = X^T\beta_2$
  - $log(\pi_3/\pi_1) = \beta_{3,0} + \beta_{3,1}*X_1 + ... + \beta_{3,p}*X_p = X^T\beta_3$
  - ...
  - $log(\pi_K/\pi_1) = \beta_{K,0} + \beta_{K,1}*X_1 + ... + \beta_{K,p}*X_p = X^T\beta_K$
- Where $\pi_i$ is the probability that a given observation is in category i and category 1 is our reference category.
- The odds that a given observation is in category i vs category 1 is given by $exp(\beta_{2,0} + \beta_{2,1}*X_1 + ... + \beta_{2,p}*X_p)$.
- Since $\sum_{j=1}^{K}\pi_j = 1$, $\pi_i = \pi_i/(\sum_{j=1}^{K}\pi_j) = (\pi_i/\pi_1)/(\sum_{j=1}^{K}\pi_j/\pi_1) = exp(X^T\beta_i)/(1+\sum_{j=2}^{K}exp(X^T\beta_j))$.
- Or if i = 1, $\pi_i = (\pi_1/\pi_1)/(1+\sum_{j=2}^{K}exp(X^T\beta_j)) = 1/(1+\sum_{j=2}^{K}exp(X^T\beta_j))$

## Interpretation of Parameter Estimates

- Let $\beta_{i,j}$ be the parameter corresponding the the coefficient of $X_j$ in the ith logistic regression model.
- a unit increase in $X_j$ results in a total increase in the log-odds by $\beta_{i,j}$. (If all other explanatory variables are kept constant)
- Similar to Logistic Regression, it is easier to interpret odds than log odds.
- For a unit increase in $X_j$, the odds that a given observation is category i vs our reference category changes by a factor of $exp(\beta_{i,j})$.




## Small example

## Multiomial Regression in R
```{r}
# Loading necessary libraries and the dataset
library(nnet)
library(caret)
library(readr)
library(dplyr)
 
steel_faults <- read_csv("steel_faults.csv")
 
# We need to combine the various fault columns into one column, and get rid of the separate columns
steel_faults <- steel_faults %>%
 mutate(Fault_Type = case_when(
   Pastry == 1 ~ "Pastry",
   Z_Scratch == 1 ~ "Z_Scratch",
   K_Scatch == 1 ~ "K_Scatch",
   Stains == 1 ~ "Stains",
   Dirtiness == 1 ~ "Dirtiness",
   Bumps == 1 ~ "Bumps",
   Other_Faults == 1 ~ "Other"
 )) %>%
 select(-Pastry, -Z_Scratch, -K_Scatch, -Stains, -Dirtiness, -Bumps, -Other_Faults)
 
 
# We need to convert the targets to factors since we are dealing with categories
steel_faults$Fault_Type <- as.factor(steel_faults$Fault_Type)
 
```
The data we are using for our main example is a dataset that contains various physical and visual measurements of steel plates. These measurements are then used to classify the fault that has occurred in the manufacturing of each plate. 
 
The types of faults that we will be classifying are:
 
- Pastry
- Z Scratch
- K Scratch
- Stains
- Dirtiness
- Bumps
- Other Faults
 
The data was found at https://archive.ics.uci.edu/dataset/198/steel+plates+faults. In our example our goal will to be to build a multinomial regression model that can accurately classify the types of faults that the plates in the dataset have.
 
## Descriptive Statistics
 
Our dataset contains 27 different predictor variables that describe the features of the fault such as size, placement, and luminosity of the faults, and other information such as the type and thickness of the steel that the plate is made of.

Here we can see the distribution of the types of faults in our dataset. 
```{r}
table(steel_faults$Fault_Type)
```
We have a total of 1940 observations of steel plate faults in our dataset.
 

## Building the full model

```{r}
library(MASS)
library(nnet)
```

 
```{r, echo=TRUE}

# Fitting the multinomial model to the data
full_model <- multinom(Fault_Type ~ ., data = steel_faults)

# Storing model summary so we can access various pieces of it 
full_model_summary <- summary(full_model)

# Accessing the AIC of the full model
full_model_summary$AIC
```

## Confusion Matrix and Interpretations
```{r}
# Find predicted values that the model suggests
predicted_full_model <- predict(full_model, newdata = steel_faults)

# Confusion matrix
cm <- confusionMatrix(predicted_full_model, steel_faults$Fault_Type)
cm$overall
cm$table
```


## Confusion Matrix and Interpretations (cont.)
```{r}
cm$byClass
```


## Finding Which Predictors to Use

```{r}
# Predictor variable correlation matrix
correlations <- cor(steel_faults %>% select_if(is.numeric))
 
# Predictor variable correlation plot
library(corrplot)
corrplot(correlations, method = "circle")
```

## Finding Which Predictors to Use (cont.)

```{r, echo=TRUE}
# Stepwise variable selection
#stepwise_model <- stepAIC(full_model, direction = "both", trace = FALSE)

# See which predictors were selected
#summary(stepwise_model)
```
In the stepwise summary, we are able to see that the model with the lowest AIC is one that contains 20 of our predictor variables. 


## Creating Refined Model Using AIC Comparisons
```{r, results='hide', echo=TRUE}
# Fitting the refined multinomial model
refined_model <- multinom(formula = Fault_Type ~ X_Minimum + Y_Minimum + Y_Maximum + 
    Minimum_of_Luminosity + Maximum_of_Luminosity + Length_of_Conveyer + 
    TypeOfSteel_A300 + TypeOfSteel_A400 + Steel_Plate_Thickness + 
    Edges_Index + Empty_Index + Square_Index + Edges_X_Index + 
    Edges_Y_Index + Log_X_Index + Log_Y_Index + Orientation_Index + 
    Luminosity_Index + SigmoidOfAreas + Outside_Global_Index, 
    data = steel_faults)

# Storing model summary so we can access various pieces of it 
refined_model_summary <- summary(refined_model)

# Accessing AIC of the refined model
refined_model_summary$AIC

```

## Updated Confusion Matrix and Interpretations
```{r}
# Find predicted values that the model suggests
predicted_refined_model <- predict(refined_model, newdata = steel_faults)

cm_refined <- confusionMatrix(predicted_refined_model, steel_faults$Fault_Type)
cm_refined$overall
cm_refined$table
```

## Updated Confusion Matrix and Interpretations (cont.)
```{r}
cm_refined$byClass
```

## References
- https://bookdown.org/sarahwerth2024/CategoricalBook/multinomial-logit-regression-r.html
- http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/
