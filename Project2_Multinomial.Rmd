---
title: "Multinomial Regression"
author: "Chris Ryan, Jeb Brown, Varsha Manickam, Meghna Kommoju, Zhennan Shen"
date: "2025-04-22"
fontsize: 9pt
classoption: "aspectratio=169"
output: 
  beamer_presentation: 
    fonttheme: professionalfonts
    highlight: kate
    theme: Rochester
header-includes:
- \definecolor{VTmaroon}{HTML}{861F41}
- \usecolortheme[named=VTmaroon]{structure}
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE,
                      comment = NA,
                      warning = FALSE,
                      message = FALSE)

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  paste0("\n \\", "footnotesize","\n\n", x, "\n\n \\normalsize")
})


```
## Introduction

> "All models are wrong, but some are useful."  
> — George E. P. Box

- In manufacturing, it’s important to find problems early to keep products high quality.
- We are using **Multinomial Logistic Regression** to predict **types of faults** in steel plates using data from the production process.
- The outcome has **more than two categories** with no specific order, which fits this model well.
- Our goal is to build a model that predicts fault types, explains which features matter most, and checks that the model works correctly.

## Multinomial Regression

Multinomial Regression (also called Multinomial Logistic Regression or Softmax Regression) is a classification method that is used to predict nominal outcome variables that have more than two outcomes (polytomous) that do not have a rank or order. Examples of this would be:

- Classifying flower species (setosa, virginica, veritosa)
- Classifying student programs (academic, vocation, general)
- Classifying student majors (engineering, business, science)

## Assumptions for Multinomial Regression

-   Independence of observations
-   Categories of the outcome variable must be mutually exclusive and exhaustive
-   No multicollinearity between independent variables
-   Linear relationship between continuous variables and the logit transformation of the outcome variable

## Multinomial Regression Basics

Multinomial Regression on a response variable with K categories and P explanatory variables is comprised of K-1 deperate Simple Logistic Regressions modeling the relationship between the log-odds of each category vs one base or "reference" category. 

-   An extension of Logistic Regression
-   Uses separate Logistic Regression models for each pair of response categories
-   Say there are K Response categories. We choose 1 base or "reference" level and then create K-1 difference Logistic Regression Models comparing
-   Each of these models will model the log-odds that an observation that an observation is in a given category vs the reference category
- The reference category can be any of the categories mathematically and you will get the same results. But it can strategically be picked for better interpretability.

## Multinomial Regression Interpretation

A Multinomial Regression model with K response categories and P explanatory variables is a collection of K-1 Logistic Regression Models:
$\begin{cases}$
$log(\pi_1/\pi_K) = \beta_{1,0} + \beta_{1,1}*X_1 + ... + \beta_{1,p}*X_p = X^T\beta_1 \\$
$log(\pi_2/\pi_K) = \beta_{2,0} + \beta_{2,1}*X_1 + ... + \beta_{2,p}*X_p = X^T\beta_2 \\$
$... \\$
$log(\pi_{K-1}/\pi_K) = \beta_{K-1,0} + \beta_{K-1,1}*X_1 + ... + \beta_{K-1,p}*X_p = X^T\beta_{K-1}$
$\end{cases}$

Where $\pi_i$ is the probability that a given observation is in category i and, by convention, category K is our reference category. 

##

As you can see, the odds that a given observation is in category i vs category K is given by $exp(X^T\beta_i)$ and the the percentage that a given observation is in category i, $\pi_i$, is $\pi_Kexp(X^T\beta_i)$. As a result, one can deduce that since the sum of the probabilities that observation i is in each category equal 1 ($\sum_{i=1}^{K}\pi_i = 1$), and $\sum_{i=1}^{K}\pi_i = \pi_K + \sum_{i=1}^{K-1}\pi_Kexp(X\beta_i) = \pi_K(1 + sum_{i=1}^{K-1}exp(X\beta_i))$, $\pi_K = 1/(1 + \sum_{i=1}^{K-1}exp(X^T\beta_i))$. Therefore, the probability that an observation is in each category is given by:
$\begin{cases}$
$\pi_1 = exp(X^T\beta_1)/(1 + \sum_{i=1}^{K-1}exp(X^T\beta_i)) \\$
$... \\$
$\pi_{K-1} = exp(X^T\beta_{K-1})/(1 + \sum_{i=1}^{K-1}exp(X^T\beta_i)) \\$
$\pi_{K} = 1/(1 + \sum_{i=1}^{K-1}exp(X^T\beta_i))$
$\end{cases}$

## Softmax function

The Softmax function is essentially an extension of the logistic function from ordinary logistic regression. The softmax function takes in a vector of log odds and converts it into a vector of probabilities. Given a vector z of length n, $softmax(z) = (1/\sum_{i=1}^{n}(exp(z_i)))[exp(z_1) ... exp(z_n)]^T$.

Now if we define another vector, $\beta_K$, equal to the zero vector, it is easy to see that $exp(X^T\beta_K) = 1$. We can then let $\pi_K = exp(X^T\beta_K)/(\sum_{i=1}^{K}exp(X^T\beta_i))$ and in general, $\pi_i = exp(X^T\beta_i)/(\sum_{j=1}^{K}exp(X^T\beta_j))$ for $i \in \{1,...,K\}$. Let's then define $\beta = [\beta_1, \beta_2,...,\beta_K] \in \mathbb{R}^{P{\times}K}$ and $\pi = [\pi_1, ..., \pi_K]^T$. Observe that $\beta^TX = [X^T\beta_1, X^T\beta_2, ..., X^T\beta_K]^T$, meaning, our model can be expressed simply as $\pi = softmax(\beta^TX)$

## Non-Uniqueness of Parameter Estimates

Lets say we update each $\beta_i$ to $\beta_i + \beta^*$ for some constant vector $\beta^*$. Let $pi_i^*$ be the updated probability that an observation is in category i given the updated parameters. Observe that, $\pi_i^* = exp(X^T(\beta_i + \beta^*))/(\sum_{j=1}^{K}exp(X^T(\beta_j + \beta^*))) = exp(X^T\beta_i)exp(X^T\beta^*)/(\sum_{j=1}^{K}exp(X^T\beta_j)exp(X^T\beta^*)) = exp(X^T\beta_i)/(\sum_{j=1}^{K}exp(X^T\beta_j)) = \pi_i$. Therefore, as we can see, the probabilities are not uniquely defined by the set of parameters and in fact, for any model if you add another vector to each of the parameter vectors, you will achieve the same results. This is why we choose a reference category and set it's parameter vector to the zero vector which in turn restricts our solution space.

## Interpretation of Parameter Estimates

- Let $\beta_{i,j}$ be the parameter corresponding the the coefficient of $X_j$ in the ith logistic regression model.
- a unit increase in $X_j$ results in a total increase in the log-odds by $\beta_{i,j}$. (If all other explanatory variables are kept constant)
- Similar to Logistic Regression, it is easier to interpret odds than log odds.
- For a unit increase in $X_j$, the odds that a given observation is category i vs our reference category changes by a factor of $exp(\beta_{i,j})$.

## Parameter Estimation

Like ordinary logistic regression, in order to estimate the parameters, $\beta$, we have to calculate the maximum likelihood estimation.

For starters, our likelihood function is given by, $L(\beta|X) = \prod_{i=1}^{n}\prod_{c=1}^{K}P(y_i=c|x_i,\beta)^{1\{y_i=c\}}$. Where $1\{\}$ is the indicator function, which returns 1 if the statement inside the brackets is true and 0 if the statement inside the brackets is false. Of course, it is typically easier to differentiate a sum rather than a product so we look at the log-likelihood function instead given by, $\ell(\beta|X) = \sum_{i=1}^n\sum_{c=1}^K1\{y_i=c\}log(P(y_i=c|x_i,\beta) = \sum_{i=1}^n\sum_{c=1}^K1\{y_i=c\}(x_i^T\beta_c - log(\sum_{j=1}^Kexp(x_i^T\beta_j)$.

##

Now, finding the maximum likelihood estimation is equivalent to finding when each $\nabla_{\beta_i}\ell(\beta|X)$ is equal to the zero vector. It can be shown that 
$\nabla_{\beta_i}\ell(\beta|X) = X^T \begin{bmatrix}1\{y_1=m\}-P(y_1=m|x_1,\beta) \\ ... \\ 1\{y_n=m\}-P(y_n=m|x_n,\beta)\end{bmatrix}$ which, like ordinary simple regression, has no closed form when set equal to 0.

Therefore we need to use a generalization of newtons method in order to find the minimum of $\nabla_{\beta_i}\ell(\beta|X)$, using the following update formula: $\beta_m^{(r)} = \beta_m^{(r-1)} - (H_m^{(r-1)})^{-1}X^T\begin{bmatrix}1\{y_1=m\}-P(y_1=m|x_1,\beta^{(r-1)}) \\ ... \\ 1\{y_n=m\}-P(y_n=m|x_n,\beta^{(r-1)})\end{bmatrix}$ Where $H_m^{(r-1)}$ is the Hessian of $\nabla_{\beta_m}\ell(\beta^{(r-1)}|X)$ and can be explicitly defined as $-X^TW_m^{(r-1)}X$ where $W_m$ is an n by n matrix with the ith diagonal entry being $P(y_i = m|x_i,\beta^{(r-1)})(1 - P(y_i = m|x_i,\beta^{(r-1)}))$

## Why Significance Test in Multinomial Regression?

- Helps determine which predictors are statistically meaningful.
- Prevents overfitting by identifying unhelpful variables.
- **Wald Test**: Used to test individual predictor coefficients.
  - But: Wald tests can be unstable, especially with small samples or multicollinearity.
- **Likelihood Ratio Test (LRT)** is often more reliable.

## Likelihood Ratio Test

- Compares a full model (with all predictors) to a reduced model (with some predictors removed).
- **Null hypothesis**: The reduced model fits the data just as well as the full model.
- **Alternative hypothesis**: The full model fits significantly better than the reduced model.
- A small *p*-value → removed predictors are important.

## LRT Example on `iris` Dataset

```{r, message=FALSE, warning=FALSE}
library(nnet)
full_model <- multinom(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris)
reduced_model <- multinom(Species ~ Petal.Length + Petal.Width, data = iris)

lrt_result <- anova(reduced_model, full_model, test = "Chisq")
lrt_result
```
- LR stat = 8.68, df = 4, p-value = 0.0696
- p > 0.05 → not statistically significant
- Sepal predictors may not be needed, reduced model performs nearly as well

## Small example

## Main example

## Multiomial Regression in R
```{r eval=FALSE, include=FALSE}

# Loading necessary libraries and the dataset
library(nnet)
library(caret)
library(readr)
library(dplyr)
 
steel_faults <- read_csv("steel_faults.csv")
 
# We need to combine the various fault columns into one column, and get rid of the separate columns
steel_faults <- steel_faults %>%
 mutate(Fault_Type = case_when(
   Pastry == 1 ~ "Pastry",
   Z_Scratch == 1 ~ "Z_Scratch",
   K_Scatch == 1 ~ "K_Scatch",
   Stains == 1 ~ "Stains",
   Dirtiness == 1 ~ "Dirtiness",
   Bumps == 1 ~ "Bumps",
   Other_Faults == 1 ~ "Other"
 )) %>%
 select(-Pastry, -Z_Scratch, -K_Scatch, -Stains, -Dirtiness, -Bumps, -Other_Faults)
 
 
# We need to convert the targets to factors since we are dealing with categories
steel_faults$Fault_Type <- as.factor(steel_faults$Fault_Type)
 
```
The data we are using for our main example is a dataset that contains various physical and visual measurements of steel plates. These measurements are then used to classify the fault that has occurred in the manufacturing of each plate. 
 
The types of faults that we will be classifying are:
 
- Pastry
- Z Scratch
- K Scratch
- Stains
- Dirtiness
- Bumps
- Other Faults
 
The data was found at https://archive.ics.uci.edu/dataset/198/steel+plates+faults. In our example our goal will to be to build a multinomial regression model that can accurately classify the types of faults that the plates in the dataset have.
 
## Descriptive Statistics
 
Our dataset contains 27 different predictor variables that describe the features of the fault such as size, placement, and luminosity of the faults, and other information such as the type and thickness of the steel that the plate is made of.

Here we can see the distribution of the types of faults in our dataset. 
```{r}
table(steel_faults$Fault_Type)
```
We have a total of 1940 observations of steel plate faults in our dataset.
 

## Building the full model

```{r}
library(MASS)
library(nnet)
```

 
```{r, echo=TRUE}

# Fitting the multinomial model to the data
full_model <- multinom(Fault_Type ~ ., data = steel_faults)

# Storing model summary so we can access various pieces of it 
full_model_summary <- summary(full_model)

# Accessing the AIC of the full model
full_model_summary$AIC
```

## Confusion Matrix and Interpretations
```{r}
# Find predicted values that the model suggests
predicted_full_model <- predict(full_model, newdata = steel_faults)

# Confusion matrix
cm <- confusionMatrix(predicted_full_model, steel_faults$Fault_Type)
cm$overall
cm$table
```


## Confusion Matrix and Interpretations (cont.)
```{r}
cm$byClass
```


## Finding Which Predictors to Use

```{r}
# Predictor variable correlation matrix
correlations <- cor(steel_faults %>% select_if(is.numeric))
 
# Predictor variable correlation plot
library(corrplot)
corrplot(correlations, method = "circle")
```

## Finding Which Predictors to Use (cont.)

```{r, echo=TRUE}
# Stepwise variable selection
#stepwise_model <- stepAIC(full_model, direction = "both", trace = FALSE)

# See which predictors were selected
#summary(stepwise_model)
```
In the stepwise summary, we are able to see that the model with the lowest AIC is one that contains 20 of our predictor variables. 


## Creating Refined Model Using AIC Comparisons
```{r, results='hide', echo=TRUE}
# Fitting the refined multinomial model
refined_model <- multinom(formula = Fault_Type ~ X_Minimum + Y_Minimum + Y_Maximum + 
    Minimum_of_Luminosity + Maximum_of_Luminosity + Length_of_Conveyer + 
    TypeOfSteel_A300 + TypeOfSteel_A400 + Steel_Plate_Thickness + 
    Edges_Index + Empty_Index + Square_Index + Edges_X_Index + 
    Edges_Y_Index + Log_X_Index + Log_Y_Index + Orientation_Index + 
    Luminosity_Index + SigmoidOfAreas + Outside_Global_Index, 
    data = steel_faults)

# Storing model summary so we can access various pieces of it 
refined_model_summary <- summary(refined_model)

# Accessing AIC of the refined model
refined_model_summary$AIC

```

## Updated Confusion Matrix and Interpretations
```{r}
# Find predicted values that the model suggests
predicted_refined_model <- predict(refined_model, newdata = steel_faults)

cm_refined <- confusionMatrix(predicted_refined_model, steel_faults$Fault_Type)
cm_refined$overall
cm_refined$table
```

## Updated Confusion Matrix and Interpretations (cont.)
```{r}
cm_refined$byClass
```

## Assumption Checks: VIF, Tolerance, Condition Index
To ensure that each coefficient in our multinomial model is meaningful, we use:

### 1. Variance Inflation Factor (VIF)

- Measures how much the variance of a coefficient \( \hat{\beta}_j \) increases due to correlation among predictors.
- If predictor \( X_j \) is linearly predicted from other variables with \( R_j^2 \), then:

\[
\text{VIF}_j = \frac{1}{1 - R_j^2}
\]

- A high \( R_j^2 \) means \( X_j \) is redundant.
- **If VIF > 5 or 10**, the variable is considered multicollinear.


### 2. Tolerance

- Simply the inverse of VIF:

\[
\text{Tolerance}_j = 1 - R_j^2 = \frac{1}{\text{VIF}_j}
\]

- **If Tolerance < 0.1**, the variable provides little new information and may cause instability.

### 3. Condition Index (CI)
- Derived from eigenvalues \( \lambda \) of the scaled \( X^TX \) matrix.
\[
\text{CI}_k = \sqrt{\frac{\lambda_{\text{max}}}{\lambda_k}}
\]

# Why This Matters in Multinomial Logistic Regression

- In multinomial regression, we estimate \( K-1 \) sets of coefficients \( \beta_i \) for each non-reference category.
- Each set is estimated using **maximum likelihood**, which assumes stable and independent information from predictors.
- If predictors are highly correlated:
  - The model may assign unstable or misleading weights to them.
  - Standard errors will be large, making **significance tests unreliable**.
  - The optimization (Newton-Raphson updates) may fail to converge or return inflated coefficients.

---

Checking multicollinearity ensures:
- The **log-odds interpretation** of each \( \beta_{i,j} \) is trustworthy.
- The probabilities returned by the **softmax function** are stable.
- The model doesn’t **overfit noise** from redundant features.
## Softmax Function: Turning Scores into Probabilities

In multinomial logistic regression, each class is assigned a **linear score**:

\[
z_k = x^\top \beta_k
\]

The **softmax function** transforms these raw scores into probabilities:

\[
\pi_k = \frac{\exp(x^\top \beta_k)}{\sum_{j=1}^{K} \exp(x^\top \beta_j)}
\]

- Each \( \pi_k \) represents the **predicted probability** of class \( k \)
- The sum of all class probabilities is 1
- This allows for a **probabilistic interpretation** of the model output

---

**Why it matters:**  
- Without softmax, the model produces unbounded values that can’t be interpreted directly.
- Softmax ensures we can apply **maximum likelihood estimation**, interpret coefficients meaningfully, and make class predictions using.

## Smaple example link to Checking Assumptions




## References
- https://bookdown.org/sarahwerth2024/CategoricalBook/multinomial-logit-regression-r.html
- http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/

- UCLA Institute for Digital Research and Education (IDRE). *Multinomial Logistic Regression | Stata/Logistic Regression Resources*.  
  Retrieved from: [https://stats.oarc.ucla.edu/](https://stats.oarc.ucla.edu/)

- Fox, J., & Weisberg, S. (2019). *An R Companion to Applied Regression* (3rd ed.).  
  Retrieved from: [https://cran.r-project.org/web/packages/car/index.html](https://cran.r-project.org/web/packages/car/index.html)

- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning* (2nd ed.). Springer.  
  Chapter 3 covers multicollinearity and variable selection.
